{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psychrnn\n",
    "from psychrnn.tasks import rdm as rd\n",
    "from psychrnn.backend.models.basic import Basic\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define params globally first before passing to RDM, mess w/ model params and explain what can do. (can turn on and off dale's law). some masking, input output connectivity. train_params variables stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10 # time-step\n",
    "tau = 100 # intrinsic time constant of neural state decay\n",
    "T = 2000 # time to run for (number of steps is T/dt)\n",
    "N_batch = 50 # number of trials per training step\n",
    "N_rec = 50 # number of recurrent units\n",
    "name = 'basicModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = rd.RDM(dt = dt, tau = tau, T = T, N_batch = N_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDM puts the params passed in as well as other generated params into a dict we can then use to create our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N_batch': 50, 'N_in': 2, 'N_out': 2, 'dt': 10, 'tau': 100, 'T': 2000, 'alpha': 0.1, 'N_steps': 200, 'coherence': None}\n"
     ]
    }
   ],
   "source": [
    "params = rdm.__dict__\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate *N_batch* trials to be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = rdm.batch_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add in a few params that Basic(RNN) needs but that RDM doesn't generate for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['name'] = name #Used to scope out a namespace for global variables.\n",
    "params['N_rec'] = N_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other optional parameters we can add in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['dale_ratio'] = None # Default: None -- when the dale_ratio is set, dale's law is applied\n",
    "params['rec_noise'] = 0.0 # Default: 0.0 -- how much noise to add to the new_state calculation\n",
    "params['W_in_train'] = True # Indicates whether W_in is trainable. Default: True\n",
    "params['W_rec_train'] = True # Indicates whether W_rec is trainable. Default: True\n",
    "params['W_out_train'] = True # Indicates whether W_out is trainable. Default: True\n",
    "params['b_rec_train'] = True # Indicates whether b_rec is trainable. Default: True\n",
    "params['b_out_train'] = True # Indicates whether b_out is trainable. Default: True\n",
    "params['init_state_train'] = True # Indicates whether init_state is trainable. Default: True\n",
    "params['load_weights_path'] = None # When given a path, loads weights from file in that path. Default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N_batch': 50, 'N_in': 2, 'N_out': 2, 'dt': 10, 'tau': 100, 'T': 2000, 'alpha': 0.1, 'N_steps': 200, 'coherence': None, 'name': 'basicModel', 'N_rec': 50, 'dale_ratio': None, 'rec_noise': 0.0, 'W_in_train': True, 'W_rec_train': True, 'W_out_train': True, 'b_rec_train': True, 'b_out_train': True, 'init_state_train': True, 'load_weights_path': None, 'parent': True}\n"
     ]
    }
   ],
   "source": [
    "basicModel = Basic(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a tensorflow session with loss, regularization, predictions, and regularized loss defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModel.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training parameters for our model. All of the parameters below are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {}\n",
    "train_params['save_weights_path'] =  '../weights/refactor_weights.npz' # Where to save the model after training. Default: None\n",
    "train_params['training_iters'] = 100000 # number of iterations to train for Default: 10000\n",
    "train_params['learning_rate'] = .001 # Sets learning rate if use default optimizer Default: .001\n",
    "train_params['loss_epoch'] = 10 # Compute and record loss every 'loss_epoch' epochs. Default: 10\n",
    "train_params['verbosity'] = True # If true, prints information as training progresses. Default: True\n",
    "train_params['save_training_weights_epoch'] = 100 # save training weights every 'save_training_weights_epoch' epochs. Default: 100\n",
    "train_params['training_weights_path'] = None # where to save training weights as training progresses. Default: None\n",
    "train_params['curriculum'] = None # curriculum object, when not none, trains using curriculum learning. Replaces trial_batch_generator when not None. Default: None\n",
    "train_params['optimizer'] = tf.train.AdamOptimizer(learning_rate=train_params['learning_rate']) # What optimizer to use to compute gradients. Default: tf.train.AdamOptimizer(learning_rate=train_params['learning_rate'])\n",
    "train_params['clip_grads'] = True # If true, clip gradients by norm 1. Default: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 500, Minibatch Loss= 0.255080\n",
      "Iter 1000, Minibatch Loss= 0.201375\n",
      "Iter 1500, Minibatch Loss= 0.148594\n",
      "Iter 2000, Minibatch Loss= 0.132121\n",
      "Iter 2500, Minibatch Loss= 0.118618\n",
      "Iter 3000, Minibatch Loss= 0.101980\n",
      "Iter 3500, Minibatch Loss= 0.046654\n",
      "Iter 4000, Minibatch Loss= 0.042707\n",
      "Iter 4500, Minibatch Loss= 0.032589\n",
      "Iter 5000, Minibatch Loss= 0.021023\n",
      "Iter 5500, Minibatch Loss= 0.053211\n",
      "Iter 6000, Minibatch Loss= 0.018797\n",
      "Iter 6500, Minibatch Loss= 0.018563\n",
      "Iter 7000, Minibatch Loss= 0.024383\n",
      "Iter 7500, Minibatch Loss= 0.018373\n",
      "Iter 8000, Minibatch Loss= 0.018971\n",
      "Iter 8500, Minibatch Loss= 0.016528\n",
      "Iter 9000, Minibatch Loss= 0.016460\n",
      "Iter 9500, Minibatch Loss= 0.012289\n",
      "Iter 10000, Minibatch Loss= 0.009562\n",
      "Iter 10500, Minibatch Loss= 0.015233\n",
      "Iter 11000, Minibatch Loss= 0.010238\n",
      "Iter 11500, Minibatch Loss= 0.013757\n",
      "Iter 12000, Minibatch Loss= 0.009833\n",
      "Iter 12500, Minibatch Loss= 0.010497\n",
      "Iter 13000, Minibatch Loss= 0.008870\n",
      "Iter 13500, Minibatch Loss= 0.011842\n",
      "Iter 14000, Minibatch Loss= 0.008250\n",
      "Iter 14500, Minibatch Loss= 0.022335\n",
      "Iter 15000, Minibatch Loss= 0.013535\n",
      "Iter 15500, Minibatch Loss= 0.015442\n",
      "Iter 16000, Minibatch Loss= 0.011969\n",
      "Iter 16500, Minibatch Loss= 0.006958\n",
      "Iter 17000, Minibatch Loss= 0.010487\n",
      "Iter 17500, Minibatch Loss= 0.009264\n",
      "Iter 18000, Minibatch Loss= 0.007653\n",
      "Iter 18500, Minibatch Loss= 0.007527\n",
      "Iter 19000, Minibatch Loss= 0.007739\n",
      "Iter 19500, Minibatch Loss= 0.011977\n",
      "Iter 20000, Minibatch Loss= 0.008651\n",
      "Iter 20500, Minibatch Loss= 0.014029\n",
      "Iter 21000, Minibatch Loss= 0.008775\n",
      "Iter 21500, Minibatch Loss= 0.009636\n",
      "Iter 22000, Minibatch Loss= 0.007641\n",
      "Iter 22500, Minibatch Loss= 0.007643\n",
      "Iter 23000, Minibatch Loss= 0.006847\n",
      "Iter 23500, Minibatch Loss= 0.006918\n",
      "Iter 24000, Minibatch Loss= 0.007426\n",
      "Iter 24500, Minibatch Loss= 0.005933\n",
      "Iter 25000, Minibatch Loss= 0.006225\n",
      "Iter 25500, Minibatch Loss= 0.006818\n",
      "Iter 26000, Minibatch Loss= 0.007857\n",
      "Iter 26500, Minibatch Loss= 0.005730\n",
      "Iter 27000, Minibatch Loss= 0.005442\n",
      "Iter 27500, Minibatch Loss= 0.004739\n",
      "Iter 28000, Minibatch Loss= 0.005618\n",
      "Iter 28500, Minibatch Loss= 0.005866\n",
      "Iter 29000, Minibatch Loss= 0.004958\n",
      "Iter 29500, Minibatch Loss= 0.005691\n",
      "Iter 30000, Minibatch Loss= 0.005255\n",
      "Iter 30500, Minibatch Loss= 0.005104\n",
      "Iter 31000, Minibatch Loss= 0.005186\n",
      "Iter 31500, Minibatch Loss= 0.005083\n",
      "Iter 32000, Minibatch Loss= 0.005489\n",
      "Iter 32500, Minibatch Loss= 0.004873\n",
      "Iter 33000, Minibatch Loss= 0.004737\n",
      "Iter 33500, Minibatch Loss= 0.005138\n",
      "Iter 34000, Minibatch Loss= 0.004561\n",
      "Iter 34500, Minibatch Loss= 0.006124\n",
      "Iter 35000, Minibatch Loss= 0.004537\n",
      "Iter 35500, Minibatch Loss= 0.004711\n",
      "Iter 36000, Minibatch Loss= 0.004712\n",
      "Iter 36500, Minibatch Loss= 0.004650\n",
      "Iter 37000, Minibatch Loss= 0.004920\n",
      "Iter 37500, Minibatch Loss= 0.004688\n",
      "Iter 38000, Minibatch Loss= 0.005036\n",
      "Iter 38500, Minibatch Loss= 0.004222\n",
      "Iter 39000, Minibatch Loss= 0.004622\n",
      "Iter 39500, Minibatch Loss= 0.004603\n",
      "Iter 40000, Minibatch Loss= 0.004360\n",
      "Iter 40500, Minibatch Loss= 0.004524\n",
      "Iter 41000, Minibatch Loss= 0.004241\n",
      "Iter 41500, Minibatch Loss= 0.004497\n",
      "Iter 42000, Minibatch Loss= 0.004194\n",
      "Iter 42500, Minibatch Loss= 0.004373\n",
      "Iter 43000, Minibatch Loss= 0.004040\n",
      "Iter 43500, Minibatch Loss= 0.004216\n",
      "Iter 44000, Minibatch Loss= 0.004421\n",
      "Iter 44500, Minibatch Loss= 0.004505\n",
      "Iter 45000, Minibatch Loss= 0.003879\n",
      "Iter 45500, Minibatch Loss= 0.004001\n",
      "Iter 46000, Minibatch Loss= 0.003937\n",
      "Iter 46500, Minibatch Loss= 0.003971\n",
      "Iter 47000, Minibatch Loss= 0.003841\n",
      "Iter 47500, Minibatch Loss= 0.003999\n",
      "Iter 48000, Minibatch Loss= 0.003914\n",
      "Iter 48500, Minibatch Loss= 0.003961\n",
      "Iter 49000, Minibatch Loss= 0.003813\n",
      "Iter 49500, Minibatch Loss= 0.003814\n",
      "Iter 50000, Minibatch Loss= 0.004117\n",
      "Iter 50500, Minibatch Loss= 0.004038\n",
      "Iter 51000, Minibatch Loss= 0.003908\n",
      "Iter 51500, Minibatch Loss= 0.004038\n",
      "Iter 52000, Minibatch Loss= 0.003881\n",
      "Iter 52500, Minibatch Loss= 0.004291\n",
      "Iter 53000, Minibatch Loss= 0.003617\n",
      "Iter 53500, Minibatch Loss= 0.003728\n",
      "Iter 54000, Minibatch Loss= 0.003738\n",
      "Iter 54500, Minibatch Loss= 0.003655\n",
      "Iter 55000, Minibatch Loss= 0.003716\n",
      "Iter 55500, Minibatch Loss= 0.004234\n",
      "Iter 56000, Minibatch Loss= 0.004715\n",
      "Iter 56500, Minibatch Loss= 0.004744\n",
      "Iter 57000, Minibatch Loss= 0.004317\n",
      "Iter 57500, Minibatch Loss= 0.004175\n",
      "Iter 58000, Minibatch Loss= 0.004040\n",
      "Iter 58500, Minibatch Loss= 0.003296\n",
      "Iter 59000, Minibatch Loss= 0.003659\n",
      "Iter 59500, Minibatch Loss= 0.003952\n",
      "Iter 60000, Minibatch Loss= 0.003703\n"
     ]
    }
   ],
   "source": [
    "basicModel.train(gen, train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the next trial from the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,m, _ = next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the x value of the trial -- for the RDM, this includes two input neurons with different coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(x[0,:,:])*dt,dt), x[0,:,:])\n",
    "plt.ylabel(\"Input Magnitude\")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.title(\"Input Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the trained model on this trial (not included in the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = basicModel.test(x)\n",
    "output = results[0]\n",
    "state_var = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(output[0,:,:])*dt,dt),output[0,:,:])\n",
    "plt.ylabel(\"Activity of Output Unit\")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.title(\"Output on New Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(state_var[0,:,:])*dt,dt),state_var[0,:,:])\n",
    "plt.ylabel(\"State Variable Value\")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.title(\"Evolution of State Variables over Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the model to clear out the tensorflow namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModel.destruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
